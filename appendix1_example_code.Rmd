---
title: 'Appendix 1: Example Code'
subtitle: "Understanding mixed effects models through data simulation"
author: "Lisa M. DeBruine & Dale J. Barr"

output: 
  html_notebook:
    toc: true
---

## Simulating data with crossed random factors

To give an overview of the simulation task, we will simulate data from a design with crossed random factors of subjects and stimuli, fit a model to the simulated data, and then try to recover the parameter values we put in from the output. In this hypothetical study, subjects classify the emotional expressions of faces as quickly as possible, and we use their response time as the primary dependent variable. Let's imagine that the faces are of two intrinsic types: either from the subject's ingroup or from an outgroup. For simplicity, we further assume that each face appears only once in the stimulus set. The key question is whether there is any difference in classification speed across the type of face.

### Required software

```{r, message=FALSE}
# load required packages
library("lme4")        # model specification / estimation
library("afex")        # anova and deriving p-values from lmer
library("broom.mixed") # extracting data from model fits 
library("tidyverse")   # data wrangling and visualisation

# ensure this script returns the same results on each run
set.seed(8675309)
```

### Establishing the data-generating parameters

```{r params-all}
# set all data-generating parameters
b0     <- 800 # intercept; i.e., the grand mean
b1     <-  50 # slope; i.e, effect of category
S0s_sd <- 100 # by-subject random intercept sd
I0i_sd <-  80 # by-item random intercept sd
S1s_sd <-  40 # by-subject random slope sd
scor   <-  .2 # correlation between intercept and slope
err_sd <- 200 # residual (error) sd
```


### Simulating the sampling process

```{r params}
# set number of subjects and items
nsubj  <- 100 # number of subjects
nitem  <- c(ingroup = 25, outgroup = 25)  # number of items
```

#### Simulate the sampling of stimulus items

```{r}
# simulate a sample of items
items <- data.frame(
  item_id = 1:sum(nitem),
  category = rep(c("ingroup", "outgroup"), nitem),
  I0i = rnorm(sum(nitem), 0, I0i_sd)
)
```


```{r}
# effect-code category
items$cat <- recode(items$category, "ingroup" = -0.5, "outgroup" = +0.5)
```


#### Simulate the sampling of subjects

```{r}
# simulate a sample of subjects

# make the correlation matrix
cormat <- matrix(c(   1, scor,
                   scor,    1), 
             nrow = 2, byrow = TRUE) 

# make a corresponding matrix of the variance 
# (multiply the SDs for each cell)
varmat <- matrix(c(S0s_sd * S0s_sd, S0s_sd * S1s_sd,
                  S0s_sd * S1s_sd, S1s_sd * S1s_sd), 
             nrow = 2, byrow = TRUE) 

# create correlated variables with the specified parameters
S <- MASS::mvrnorm(n = nsubj, mu = c(0, 0), Sigma = cormat * varmat)

subjects <- data.frame(
  subj_id = 1:nsubj,
  S0s = S[, 1],
  S1s = S[, 2]
)
```


#### Simulate trials (encounters)

```{r}
# cross subject and item IDs; add an error term
trials <- crossing(subj_id = subjects$subj_id,
                   item_id = items$item_id)  %>%
  mutate(err = rnorm(nrow(.), mean = 0, sd = err_sd))

# join subject and item tables
joined <- trials %>%
  inner_join(subjects, "subj_id") %>%
  inner_join(items, "item_id")
```

#### Calculate the response values


```{r}
# calculate the response variable
dat_sim <- joined %>%
  mutate(RT = b0 + I0i + S0s + (b1 + S1s) * cat + err) %>%
  select(subj_id, item_id, category, cat, RT)
```


### Data simulation function


```{r}
# set up the custom data simulation function
my_sim_data <- function(nsubj  = 100, # number of subjects
                        nitem  = c(ingroup = 25, outgroup = 25),  # number of items
                        b0     = 800, # grand mean
                        b1     =  50, # effect of category
                        I0i_sd =  80, # by-item random intercept sd
                        S0s_sd = 100, # by-subject random intercept sd
                        S1s_sd =  40, # by-subject random slope sd
                        scor   = 0.2, # correlation between intercept and slope
                        err_sd = 200  # residual (standard deviation)
                        ) {
  # simulate items
  items <- data.frame(
    item_id = 1:sum(nitem),
    category = rep(c("ingroup", "outgroup"), nitem),
    I0i = rnorm(sum(nitem), 0, I0i_sd)
  )

  # effect code category
  items$cat <- recode(items$category, "ingroup" = -0.5, "outgroup" = 0.5)
  
  # simulate subjects
  # make the correlation matrix
  cormat <- matrix(c(   1, scor,
                     scor,    1), 
               nrow = 2, byrow = TRUE) 
  
  # make a corresponding matrix of the variance 
  # (multiply the SDs for each cell)
  varmat <- matrix(c(S0s_sd * S0s_sd, S0s_sd * S1s_sd,
                    S0s_sd * S1s_sd, S1s_sd * S1s_sd), 
               nrow = 2, byrow = TRUE) 
  
  # create correlated variables with the specified parameters
  S <- MASS::mvrnorm(n = nsubj, mu = c(0, 0), Sigma = cormat * varmat)
  
  subjects <- data.frame(
    subj_id = 1:nsubj,
    S0s = S[, 1],
    S1s = S[, 2]
  )
  
  # simulate trials
  dat_sim <- crossing(subj_id = subjects$subj_id,
                      item_id = items$item_id) %>%
    inner_join(subjects, "subj_id") %>%
    inner_join(items, "item_id") %>%
    mutate(err = rnorm(nrow(.), mean = 0, sd = err_sd)) %>%
    mutate(RT = b0 + I0i + S0s + (b1 + S1s) * cat + err) %>%
    select(subj_id, item_id, category, cat, RT)
  
  dat_sim
}
```

## Analyzing the simulated data

```{r}
# fit a linear mixed-effects model to data
mod_sim <- lmer(RT ~ 1 + cat + (1 | item_id) + (1 + cat | subj_id),
                data = dat_sim, REML = TRUE)

summary(mod_sim, corr = FALSE)
```


```{r}
# get a tidy table of results
broom.mixed::tidy(mod_sim) %>% 
  mutate(sim = c(b0, b1, S0s_sd, S1s_sd, scor, I0i_sd, err_sd)) %>%
  select(1:3, 9, 4:8)
```


## Calculate Power

```{r}
# set up the power function
my_lmer_power <- function(...) {
  # ... is a shortcut that forwards any arguments to my_sim_data()
  dat_sim <- my_sim_data(...)
  mod_sim <- lmer(RT ~ cat + (1 | item_id) + (1 + cat | subj_id),
                dat_sim, REML = FALSE)
  
  broom.mixed::tidy(mod_sim)
}
```


```{r}
# run one model with default parameters
my_lmer_power()
```


```{r}
# run one model with new parameters
my_lmer_power(nitem = c(ingroup = 50, outgroup = 50), b1 = 20)
```


```{r, eval = FALSE}
# run simulations and save to a file
reps <- 100
sims <- purrr::map_df(1:reps, ~my_lmer_power())
write_csv(sims, "sims/sims.csv")
```

```{r, message=FALSE}
# read saved simulation data
sims <- read_csv("sims/sims.csv")
```

```{r}
# calculate mean estimates and power for specified alpha
alpha <- 0.05

sims %>% 
  filter(effect == "fixed") %>%
  group_by(term) %>%
  summarise(
    mean_estimate = mean(estimate),
    mean_se = mean(std.error),
    power = mean(p.value < alpha)
  )
```


# Comparison to ANOVA

One way many researchers would normally analyse data like this is by averaging each subject's reaction times across the ingroup and outgroup stimuli and compare them using a paired-samples t-test or ANOVA (which is formally equivalent). Here, we use `afex::aov_ez` to analyse a version of our dataset that is aggregated by subject.

```{r}
# aggregate by subject and analyze with ANOVA
dat_subj <- dat_sim %>%
  group_by(subj_id, category, cat) %>%
  summarise(RT = mean(RT))

afex::aov_ez(
  id = "subj_id",
  dv = "RT",
  within = "category",
  data = dat_subj
)
```

Alternatively, you could aggregate by item, averaging all subjects' scores for each item.

```{r}
# aggregate by item and analyze with ANOVA
dat_item <-  dat_sim %>%
  group_by(item_id, category, cat) %>%
  summarise(RT = mean(RT))

afex::aov_ez(
  id = "item_id",
  dv = "RT",
  between = "category",
  data = dat_item
)
```


We can create a power analysis function that simulates data using our data-generating process from `my_sim_data()`, creates these two aggregated datasets, and analyses them with ANOVA. We'll just return the p-values for the effect of category as we can calculate power as the percentage of these simulations that reject the null hypothesis.

```{r}
# power function for ANOVA
my_anova_power <- function(...) {
  dat_sim <- my_sim_data(...)
  
  dat_subj <-  dat_sim %>%
    group_by(subj_id, category, cat) %>%
    summarise(RT = mean(RT))
  
  dat_item <-  dat_sim %>%
    group_by(item_id, category, cat) %>%
    summarise(RT = mean(RT))

  a_subj <- afex::aov_ez(id = "subj_id",
                         dv = "RT",
                         within = "category",
                         data = dat_subj)
  suppressMessages(
    # check contrasts message is annoying
    a_item <- afex::aov_ez(
      id = "item_id",
      dv = "RT",
      between = "category",
      data = dat_item
    )
  )
  
  list(
    "subj" = a_subj$anova_table$`Pr(>F)`,
    "item" = a_item$anova_table$`Pr(>F)`
  )
}
```

Run this function with the default parameters to determine the power each analysis has to detect an effect of category of `r b1` ms.

```{r anova-power}
# run simulations and calculate power 
reps <- 100
anova_sims <- purrr::map_df(1:reps, ~my_anova_power())

alpha <- 0.05
power_subj <- mean(anova_sims$subj < alpha)
power_item <- mean(anova_sims$item < alpha)
```

The by-subjects ANOVA has power of `r power_subj`, while the by-items ANOVA has power of `r power_item`. This isn't simply a consequence of within versus between design or the number of subjects versus items, but rather a consequence of the inflated false positive rate of some aggregated analyses.

Set the effect of category to 0 to calculate the false positive rate. This is the probability of concluding there is an effect when there is no actual effect in your population.

```{r}
# run simulations and calculate the false positive rate 
reps <- 100
anova_fp <- purrr::map_df(1:reps, ~my_anova_power(b1 = 0))

false_pos_subj <- mean(anova_fp$subj < alpha)
false_pos_item <- mean(anova_fp$item < alpha)
```

Ideally, your false positive rate will be equal to alpha, which we set here at `r alpha`. The by-subject aggregated analysis has a massively inflated false positive rate of `r false_pos_subj`, while the by-item aggregated analysis has a closer-to-nominal false positive rate of `r false_pos_item`. This is not a mistake, but a consequence of averaging items and analysing a between-item factor. Indeed, this problem with false positives is one fo the most compelling reasons to analyze cross-classified data using mixed effects models.